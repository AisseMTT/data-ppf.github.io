{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: Past, Present, Future |  Lab 4  |  2/09/2017\n",
    "\n",
    "\n",
    "# The Role of Statistical Entities in Society: Least Squares, Linear Regression, and Machine Learning, oh my! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grab this notebook, go [here](data-ppf.github.io/labs/lab4/Lab4.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To grab the other needed files, execute this code block command: \n",
    "!wget https://data-ppf.github.io/labs/lab4/Residuals.jpeg\n",
    "!wget https://data-ppf.github.io/labs/lab4/Star.obs.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To ensure images in this notebook are displayed\n",
    "# properly, please execute this block of code.\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall the \"double role\" of statistics in Politics:\n",
    "1. <b>construction of statistical entities</b>: \"stable objects\" can be measured and used as forms of evidence and certainty (e.g., the GDP, unemployment, life expectation, citation indexs, etc.)\n",
    "2. <b>explication and analysis of relationships between entities</b>: what are the relationships between objects and how does changing one influence others? \n",
    "-(see Desrosieres 61)\n",
    "        \n",
    "### Central Question: Does machine learning change the role of statistical entities in society?  Or, does machine learning matter?\n",
    "\n",
    "<b>Tentative claim</b>: The form of this answer is historical, material, and technical. \n",
    "<b> Aside</b>: The field of machine learning christens old techniques as machine learning techniques (e.g., \"linear regression\" existed well before \"machine learning\"!)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Statistical Entities: \n",
    "#### Least Squares, Gaussians, and Regression\n",
    "Rather than take a contemporary issue, let's see how the investigation of the history of a particular statistical technique gives us insight into how statistical entities are created and how they are used to make arguments. \n",
    "\n",
    "#### How did linear regression become thinkable? \n",
    "What problem was least squares suppose to solve? (Hint: recall Lengendre in 1805; what was the Laplace-Gaussian synthesis circa 1810?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Empirical Tradition\n",
    "In the 18th century astronomers and physicists had a bunch of separate observations of a particular object of insterest (e.g., many observations of, say, the distance between a binary star system on different days and at different observatories) but it was not clear these observations could be combined.   \n",
    " \n",
    "For cases involving objects whose existence was not in doubt (e.g., planets, stars, etc.), an empirical tradition existed which attempted to identify the \"real\" value of an observation by minimizing the sum of the squares of the \"residuals\". To wit:\n",
    "\n",
    "\n",
    "![Residuals!](Residuals.jpeg)\n",
    "\n",
    "In general, each residual, $r_i$, can be written as\n",
    "\n",
    "$ r_i = y_i - y(x_i, \\beta_i)$\n",
    "\n",
    "which is the distance between the $i^{th}$ observation $y_i$. The \"best fit line\" is $y = \\beta_0 + \\beta_1(x)$, denoted in blue above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We know from Desrosieres that linear regression doesn't really begin to take the form we are used to till Galton and Karl Pearson arrive on the seen in the second half of the 19th century. But that gets ahead of our story: first we need to understand what the Gauss-Laplace synthesis was that Desrosieres discusses, and how this synthesis lead to the gaussian being the \"correct\" distribution to use for a range of activities *unrelated to measuring error*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Imagine we are measuring the position of a star *in only 1-dimension*, i.e., we are measuring a star only along the x-axis.\n",
    "![star_obs](Star.obs.jpeg)\n",
    "Error made things difficult: How combine multiple observations for a single object? (How solve for variables that are overdetermined?) <b>NOTE THAT THE GRAPH ABOVE IS AN EXAMPLE OF THE \"OBJECTIVE MEAN\".</b>\n",
    "\n",
    "<b>18th Century Answers to this problem:</b>\n",
    "- Average observations to reduce number of equations!\n",
    "- minimize the sum of absolute values of residuals!\n",
    "\n",
    "## Legendre's answer in 1805? (Hint: See also Gauss' answer in 1795?)\n",
    "<b>Answer: \"method of least squares\"</b> in which we\n",
    "\n",
    "$min \\sum{r_i} = min \\sum_{i = 1}^{i}{(y_i-y(x_i,\\beta_i))^2}$\n",
    " \n",
    "This was mathematically similar to linear regression performed by Karl Pearson and Galton in the 1890s, but very different interpretations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Laplace Synthesis and the Modelling of Error (i.e., the epistemic tradition)\n",
    "- Theoretical description of error: tried lines ($-mx+b$), exponentials ($exp^{-x}$), logarithms ($log(c/x)$), but gaussians ($exp^{-x^2}$) eventually favored over all other distributions for describing \"random\"/\"accidental\" error. \n",
    "\n",
    "#### Why did Gaussians eclipse other models of error?\n",
    "#### Disciplinary Aside: Gaussians in other fields..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Laplace in 1810!\n",
    "\n",
    "What did he discover? \n",
    "\n",
    "<b>The Central Limit Theorem:</b> \n",
    "0. Start with *any* distribution, known or unknown. \n",
    "1. Take $N$ samples of $X$ observations.\n",
    "2. Take the mean of $X$ observations for each sample.\n",
    "3. Plot the means of the samples.\n",
    "4. The histogram of the sample of the means will <b>tend toward a gaussian as $N \\rightarrow \\infty $\n",
    "\n",
    "...*that is, regardless of the distribution of your residuals, the means of $N$ samples of $X$ observations will always produce a gaussian distribution when one takes enough samples!*\n",
    "\n",
    "Lets verify this for ourselves! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import beta\n",
    "from scipy.stats import beta as scipy_beta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generates S samples of N randomly sampled variables\n",
    "def samples_from_beta(num_of_samples, sample_size):\n",
    "    samples = beta(.5, .5, [num_of_samples, sample_size]) #NOTE THIS DISTRIBUTION IS NOT A GAUSSIAN\n",
    "    return samples\n",
    "\n",
    "def means_of_samples(samples_from_beta, num_of_samples, sample_size):\n",
    "    means = []\n",
    "    for sample in range(num_of_samples):\n",
    "        means.extend([samples_from_beta[sample].mean()])\n",
    "    means = np.array(means)    \n",
    "    return means\n",
    "\n",
    "def gaussian_curve_overlay(data, x):\n",
    "    sigma = data.std()\n",
    "    mu = data.mean()\n",
    "    plt.plot(x, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * sigma**2) ), linewidth=2, color='r') \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A beta distribution is nothing like a gaussian!\n",
    "## Here we plot out three beta distributions (with different alphas & betas) to show you\n",
    "## what a beta distribution is and to convince you it is \n",
    "## nothing like a gaussian\n",
    "\n",
    "##Historical Aside: Karl Pearson made the beta distribution well-known to statistics.\n",
    "\n",
    "\n",
    "## Pick different $\\alphas$ and $\\betas$ for beta distribution\n",
    "#a, b = 1.0, 5.0   # pdf centered on left side\n",
    "#a, b = 2.0 5.0    # pdf skewed to left, with large right \"wing\"\n",
    "a, b = 0.5, 0.5    # pdf nothing like a gaussian\n",
    "\n",
    "#plot beta for a, b = 0.5, 0.5 \n",
    "a, b = 0.5, 0.5    # pdf nothing like a gaussian\n",
    "x = np.linspace(scipy_beta.ppf(0.0, a, b), scipy_beta.ppf(1.0, a, b), 100)\n",
    "f = scipy_beta(a, b)\n",
    "plt.plot(x, f.pdf(x), color = \"blue\")\n",
    "\n",
    "#plot beta for a, b = 2.0 5.0\n",
    "a, b = 2.0, 5.0    # pdf skewed to left, with large right \"wing\"\n",
    "x = np.linspace(scipy_beta.ppf(0.0, a, b), scipy_beta.ppf(1.0, a, b), 100)\n",
    "f = scipy_beta(a, b)\n",
    "plt.plot(x, f.pdf(x), color = \"red\")\n",
    "\n",
    "#plot beta for a, b = 1.0, 5.0\n",
    "a, b = 1.0, 5.0    # pdf skewed to left, with large right \"wing\"\n",
    "x = np.linspace(scipy_beta.ppf(0.0, a, b), scipy_beta.ppf(1.0, a, b), 100)\n",
    "f = scipy_beta(a, b)\n",
    "plt.plot(x, f.pdf(x), color = \"green\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, lets use $\\alpha = \\beta = .5$ for our beta distribution (i.e., the blue curve above) since this looks the least like a gaussian. \n",
    "\n",
    "Now lets examine a few plots so see how producing a historgram of the mean of more and more samples eventually converges to a gaussian distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For 10 samples...\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "num_of_samples = 10\n",
    "num_of_random_vars = 10\n",
    "data = means_of_samples(samples_from_beta(num_of_samples, num_of_random_vars), \\\n",
    "                        num_of_samples, num_of_random_vars)\n",
    "plt.hist(data, x, normed=True)\n",
    "gaussian_curve_overlay(data, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For 100 samples...\n",
    "x = np.linspace(0, 1, 100)\n",
    "num_of_samples = 100\n",
    "num_of_random_vars = 10\n",
    "data = means_of_samples(samples_from_beta(num_of_samples, num_of_random_vars), \\\n",
    "                        num_of_samples, num_of_random_vars)\n",
    "plt.hist(data, x, normed=True)\n",
    "gaussian_curve_overlay(data, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For 1000 samples...\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "num_of_samples = 1000\n",
    "num_of_random_vars = 10\n",
    "data = means_of_samples(samples_from_beta(num_of_samples, num_of_random_vars), \\\n",
    "                        num_of_samples, num_of_random_vars)\n",
    "plt.hist(data, x, normed=True)\n",
    "gaussian_curve_overlay(data, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For 100000 samples...\n",
    "x = np.linspace(0, 1, 100)\n",
    "num_of_samples = 100000\n",
    "num_of_random_vars = 10\n",
    "data = means_of_samples(samples_from_beta(num_of_samples, num_of_random_vars), \\\n",
    "                        num_of_samples, num_of_random_vars)\n",
    "plt.hist(data, x, normed=True)\n",
    "gaussian_curve_overlay(data, x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That looks very close to a gaussian even thought the distribution we sampled from (i.e., a beta distribution) looks nothing like a gaussian! \n",
    "\n",
    "### Laplace saw this and realized that Gauss' \"least squares approach\" could handle residual error regardless of how that error was distributed. This was an important *theoretical* argument for modeling error as a gaussian.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Intermission: Queletet and constant causes\n",
    "\n",
    "At this point, we could stop here to have a conversation (as we did in class) about how Queletet argued that one could conflate \"objective means\" and \"subjective means\" in order to discuss populations (i.e., his \"average man\"). It took nearly a century to build off of the Gauss-Laplace synthesis, largely because of the interpretative challenge of equating objective and subjective means as identical (something that happened with Pearson, Galton, and others at then end of the 19th century)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression, round 2\n",
    "- What kind of machine learning technique is linear regression? (Contemporary context)\n",
    "        - Three kinds of ML techniques\n",
    "    \n",
    "- What problem was Francis Galton and Karl Pearson were interested in solving? \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## For now I just use simulated data to perform a regression on. Future versions of this notebook will use\n",
    "## actual data. \n",
    "\n",
    "\n",
    "#generate and plot \"simulated\" data\n",
    "from numpy.random import normal\n",
    "mu, sigma = 0, 2.0\n",
    "random_x = normal(mu, sigma, 30)\n",
    "random_y = normal(2, 3, 30)\n",
    "\n",
    "#output actual list of data\n",
    "print(random_x, random_y)\n",
    "\n",
    "#plot data\n",
    "plt.scatter(random_x, random_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We can put this data into pandas, allowing us to reference\n",
    "## this data via column name\n",
    "\n",
    "d = {'x' : pd.Series(random_x),  'y' : pd.Series(random_y)}\n",
    "dat = pd.DataFrame(d)\n",
    "\n",
    "#plot data\n",
    "plt.scatter(dat[\"x\"], dat[\"y\"])\n",
    "\n",
    "#output dataframe\n",
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression IN GENERAL:\n",
    "\n",
    "$y = \\beta_n x_n + ... + \\beta_1 x_1 + \\mu_0$\n",
    "\n",
    "Linear Regression FOR JUST ONE VARIABLE:\n",
    "\n",
    "$y = \\beta_1 x_1 + \\mu$ \n",
    "\n",
    "where $\\beta_1$ is the slope, $x$ are the observations, and $\\mu$ is the y-intercept.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "sm_lm = sm.ols(data = dat, formula = \"y ~ x\").fit()\n",
    "\n",
    "print(sm_lm.summary())\n",
    "print(\"----\")\n",
    "print(sm_lm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we get:\n",
    "    \n",
    "$y = (coefficient.of.x)*x + (intercept.coefficient)$\n",
    "\n",
    "which you can easily plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from above, look up the following two coefficients and input their values below:\n",
    "# YOU MUST CHANGE THESE VALUES EACH TIME YOU GENERATE NEW DATA\n",
    "x_coefficient = 0.216519\n",
    "intercept = 1.732801\n",
    "\n",
    "plt.scatter(dat[\"x\"], dat[\"y\"])\n",
    "x = np.linspace(-5.5, 7, 100)\n",
    "plt.plot(x,x_coefficient*x + intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can do the same thing in scikit-learn and its faster and therefore more useful when handling slightly bigger data sets\n",
    "# IN THIS CASE, SKLEARN PLOTS LINE AUTOMATICALLY\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "dat_array = np.asarray(dat)\n",
    "skl_lm = linear_model.LinearRegression()\n",
    "x = dat.x.values\n",
    "x = x.reshape(len(x), 1) #note that scikit-learn requires an input-matrix of a particular shape...\n",
    "\n",
    "# now we do the above two operations for y in one line...\n",
    "y = (dat.y.values).reshape(len(dat.y.values), 1)\n",
    "\n",
    "# generate model\n",
    "skl_lm.fit(x, y)\n",
    "\n",
    "# plot fit line\n",
    "plt.scatter(x, y,  color='black')\n",
    "plt.plot(x, skl_lm.predict(x), color='blue', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
