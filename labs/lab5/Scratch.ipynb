{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b891f38c0ba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dat/pol_tweets.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtweets_2017\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m'2017-01-29'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m'2017-01-30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtweets_2017\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_2017\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'hashtag_entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('dat/pol_tweets.csv', sep=';', na_values=\".\")\n",
    "tweets[\"created_at\"] = pd.to_datetime(tweets[\"created_at\"])\n",
    "\n",
    "tweets_2017 = tweets[(tweets['created_at'] > '2017-01-29') & (tweets['created_at'] < '2017-01-30')]\n",
    "tweets_2017 = tweets_2017.loc[:,'id':'hashtag_entities']\n",
    "tweets_2017\n",
    "\n",
    "tweets_2017.to_csv('Jan_2017_tweets.csv', index = False)\n",
    "\n",
    "\n",
    "len(tweets_2017.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5c2d082f4eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# How many tweets are contained in the data set?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# How many tweets are contained in the data set?\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accounts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a496838c43c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# How many unique twitter accounts exist in this data set?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'accounts' is not defined"
     ]
    }
   ],
   "source": [
    "# How many unique twitter accounts exist in this data set?\n",
    "len(accounts.screen_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6bca305b8111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here's a list of twitter accounts associated with the above tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dat/pol_accounts.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maccounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Here's a list of twitter accounts associated with the above tweets\n",
    "\n",
    "accounts = pd.read_csv('dat/pol_accounts.csv', sep=';', na_values=\".\")\n",
    "accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0557da004ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mearliest_tweet_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"earliest tweet date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearliest_tweet_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlatest_tweet_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"latest tweet date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_tweet_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "earliest_tweet_date = tweets['created_at'].min()\n",
    "print(\"earliest tweet date\", earliest_tweet_date)\n",
    "\n",
    "latest_tweet_date = tweets['created_at'].max()\n",
    "print(\"latest tweet date\", latest_tweet_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PCA Data Parameters --------------------------\n",
    "chunk_corpus_flag = 0     # 0 = don't chunk texts; 1 = do chunk texts\n",
    "number_of_MFWs_used = 1000 #number of MFWs used to plot PCA graphs # if set to 0, will use all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prepare, Tokenize, and Count Words of Corpus -----------------------\n",
    "\n",
    "## read & tokenize all txts in data_location directory; translate txt files into list of (ordered) 1-word strings\n",
    "wordlist, textnames = ta.tokenized_texts_and_textname_list(ta.filenames_of_txts_in_directory(data_location), data_location)\n",
    "\n",
    "## generate word counts\n",
    "### word_counts: dict of individual word counts per text\n",
    "### total_word_counts: list of total word counts per text\n",
    "word_counts, total_word_counts = ta.type_counts_and_total_token_counts(wordlist)\n",
    "\n",
    "## corpus_word_count_for_chunks: total word count of corpus (same as corpus_word_count_for_chunks)\n",
    "corpus_word_count = ta.total_number_of_words_in_corpus(total_word_counts)\n",
    "\n",
    "## generate word frequencies \n",
    "word_frequencies = ta.word_freq(word_counts, corpus_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the output (i.e., wordlist, textnames, word_counts, word_frequencies, etc.) from the code block above using the code block below. If you're using your own texts, be sure to change the corpus_folder variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SAVE THE RESULTS FROM THE \"Prepare, Tokenize, and Count Words of Corpus\" code block\n",
    "## Once you calulate the above code block, you can save the data by uncommenting and executing the code below. \n",
    "\n",
    "#corpus_folder = \"9_novels\"\n",
    "#ta.export_PCA_analysis(corpus_folder, word_counts, total_word_counts, corpus_word_count, word_frequencies, wordlist, textnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### If you've saved the calculation results by executing the code block above, you can load the results into memory with the code block below.\n",
    "\n",
    "### If you have previously saved the 9_novel results with the code block above, you can reload this data \n",
    "### by executing this code block\n",
    "\n",
    "#wordcounts, total_word_counts, corpus_word_count, word_frequencies, wordlist, textnames = ta.import_PCA_analysis('51_novels')\n",
    "#chunk_corpus_flag = 0  \n",
    "#number_of_MFWs_used = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## May need to run this code block twice to get it to display properly\n",
    "\n",
    "## generate color spectrum based on word count of texts ----------\n",
    "total_tag_counts_nparray = np.array(user_total_number_of_hashtags) # convert word count list into a numpy array\n",
    "colors_for_tags = ta.assign_text_colors_via_word_counts(total_tag_counts_nparray, corpus_hashtag_count)\n",
    "\n",
    "##plot PCA without legend or tables -------\n",
    "size_of_plot = 20\n",
    "name_of_file = \"pca-2017-01-29_tweets.csv\"\n",
    "plot_PCA(pca_coordinates, pca_results, colors_for_tags, textnames, size_of_plot, 0, name_of_file)  # last three inputs are plot_size, output flag, graph name\n",
    "\n",
    "#### For reference, also generate historgram\n",
    "#histogram_of_word_counts(total_word_counts_nparray, 0, \"histo-\"+str(number_of_MFWs_used)+\"-Full\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
