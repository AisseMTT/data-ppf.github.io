{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: Past, Present, Future\n",
    "## Lab 8 part II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons, an early success, later maligned as a failure\n",
    "in the AI community, leveraging Monte Carlo methods to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The birth, death, and rebirth of the Perceptron\n",
    "\n",
    "\n",
    "The researchers McCulloch & Pitts, inspired both by Turing's\n",
    "1936 paper on universal computation but also what\n",
    "was known about neuron electophysiology at the time,\n",
    "proposed the first artificial neural net abstraction. They\n",
    "(incorrectly) claimed that such a network could embody\n",
    "what we would now call a Turing machine, capable of any\n",
    "computation.\n",
    "\n",
    "Later, the psychologist Frank Rosenblatt proposed\n",
    "a model to instantiate this in a mechanical computer -- not\n",
    "as \"software\" but as a specialized piece of electronics\n",
    "in the same way Claude Shannon's Theseus mouse was a special\n",
    "purpose computer. ![perceptron](https://i.imgur.com/Xin4gI5.png)\n",
    "\n",
    "Check out http://www.dtic.mil/docs/citations/AD0256582 for a detailed (>28MB) report.\n",
    "\n",
    "Rosenblatt considered one output that took many signals\n",
    "as input. ![logical](https://i.stack.imgur.com/1c2Mb.png)\n",
    "\n",
    "In a way this is like multivariate regression\n",
    "as studied by Yule for looking at English poverty at the\n",
    "end of the 19th century.\n",
    "\n",
    "The perceptron is an example of what is called \"supervised learning,\" because it learns to classify data based on a \"training set\" that is already classified. \n",
    "\n",
    "Using a training set of vectors that are each classfied as being on one or the other side of a line, the perceptron learns how to divide such vectors. Ye olde mandatory Wikipedia diagramme: ![wikilinearclassified](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/1224px-Perceptron_example.svg.png)\n",
    "\n",
    "He proposed a rule in which one chooses either 1 new\n",
    "example or chooses one example at random to update the\n",
    "separating hyper-plane -- the line that separates \"+\"\n",
    "from \"-\" examples as in:\n",
    "![plusminus](https://camo.githubusercontent.com/8d668489613be0b9435044f224ef32a25ae30d74/687474703a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f626c6f672f323031352f73696e676c656c617965725f6e657572616c5f6e6574776f726b735f66696c65732f70657263657074726f6e5f62696e6172792e706e67)\n",
    "\n",
    "The amount that the weights of each \"neuron\" changes\n",
    "are determined by walking down a \"hill\" in the space\n",
    "of weights, where the hill is higher if you are misclassifying\n",
    "many examples. The slope of the hill is called its gradient, \n",
    "and since this technique chooses an example at\n",
    "random it's called stochastic gradient descent.\n",
    "\n",
    "In this way we know we won't get one unique answer;\n",
    "we might get an answer that's pretty good though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, `python` has a robust perceptron implementation in `scikit learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the example of a series of x,y coordinates that are classified into two classes. We'll use a pandas data frame, cuz pandas are nice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this example adapted from http://stamfordresearch.com/python-perceptron-re-visited/\n",
    "\n",
    "A = [2, 1, 2, 5, 7, 2, 3, 6, 1, 2, 5, 4, 6, 5]\n",
    "B = [2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 7]\n",
    "classification = [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "trainingdata = pd.DataFrame({\n",
    "'A' : A ,\n",
    "'B' : B ,\n",
    "'classification' : classification\n",
    "})\n",
    "\n",
    "# Targets here means the classification of each point. So (2,2) is 0, and (2,4) is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just graph these, with the points classified as 0 and 1 in different colors\n",
    "\n",
    "colormap = np.array(['r', 'k'])\n",
    " \n",
    "# Plot the data, A is x axis, B is y axis\n",
    "# and the colormap is applied based on the classification\n",
    "plt.scatter(trainingdata.A, trainingdata.B, c=colormap[trainingdata.classification], s=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, pretty easy to anticipate that this training set of points could be classified using a line.\n",
    "\n",
    "A perceptron is one algorithm for 'learning' such a line. In scikit learn, we set up out classifier and then tell it to build a classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set up the classfier by creating the perceptron object (net)\n",
    "net = Perceptron(n_iter=100, verbose=0, random_state=None, fit_intercept=True, eta0=0.002)\n",
    " \n",
    "# Then use the training data using the perceptron object (net)\n",
    "net.fit(trainingdata[['A', 'B']],trainingdata['classification'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, what's my line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient 0 \" + str(net.coef_[0,0]))\n",
    "print(\"Coefficient 1 \" + str(net.coef_[0,1]))\n",
    "print(\"Bias \" + str(net.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and the line (the \"decision boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the original data\n",
    "plt.scatter(trainingdata.A, trainingdata.B, c=colormap[trainingdata.classification], s=40)\n",
    " \n",
    "# Calc the hyperplane (decision boundary)\n",
    "ymin, ymax = plt.ylim()\n",
    "w = net.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(ymin, ymax)\n",
    "yy = a * xx - (net.intercept_[0]) / w[1]\n",
    " \n",
    "# Plot the hyperplane\n",
    "plt.plot(xx,yy, 'k-')\n",
    "plt.ylim([0,8]) # Limit the y axis size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, based on this trained classifier, we can make predictions about other data. \n",
    "\n",
    "Let's consider (7,2) and (2,7)\n",
    "\n",
    "`net.predict` will use the trained classifier to make a prediction, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = net.predict([[7,2],[2,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted death of the perceptron\n",
    "\n",
    "Symbolic AI practitioners no friend to neural nets as a research program that might draw funding and computing time away from their efforts. Major technical effort to undermine perceptron as a research program.\n",
    "\n",
    "> In the middle 1960s Papert and Minsky set out to kill the perceptron, or, at least, to establish its limitations--a task Minsky felt was a sort of social service they could perform for the artificial intelligence community. [quoted in Olazaran, 628]\n",
    "\n",
    "Minsky noted,\n",
    "\n",
    "> Part of our drive came . . . from the fact that funding and research energy were being dissapated on . . . misleading attempt to use connectionist methods in practical applications. [ibid, 628]\n",
    "\n",
    "Other forms of pattern recognition would be the future for AI. \n",
    "\n",
    "Many critical complaints, but key one for future research:\n",
    "\n",
    "Many crucial things not linearly separable, notably the simple logical operation XOR\n",
    "\n",
    "![xor](http://www.theprojectspot.com/images/post-assets/ls.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebirth\n",
    "\n",
    "- multilayer\n",
    "\n",
    "![multilayer](https://i.imgur.com/E1Pqfbn.png)\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hacker_koan#Uncarved_block\n",
    "\n",
    "    In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.\n",
    "\n",
    "    \"What are you doing?\", asked Minsky.\n",
    "    \"I am training a randomly wired neural net to play Tic-tac-toe\", Sussman replied.\n",
    "    \"Why is the net wired randomly?\", asked Minsky.\n",
    "    \"I do not want it to have any preconceptions of how to play\", Sussman said.\n",
    "\n",
    "    Minsky then shut his eyes.\n",
    "    \"Why do you close your eyes?\" Sussman asked his teacher.\n",
    "    \"So that the room will be empty.\"\n",
    "    At that moment, Sussman was enlightened. \n",
    "    \n",
    "Story told in Levy, *Hackers*\n",
    "    \n",
    "    So Sussman began working on a program. Not long after, this odd-looking bald guy came over. Sussman figured the guy was going to boot him out, but instead the man sat down, asking, \"Hey, what are you doing?\" Sussman talked over his program with the man, Marvin Minsky. At one point in the discussion, Sussman told Minsky that he was using a certain randomizing technique in his program because he didn't want the machine to have any preconceived notions. Minsky said, \"Well, it has them, it's just that you don't know what they are.\" It was the most profound thing Gerry Sussman had ever heard. And Minsky continued, telling him that the world is built a certain way, and the most important thing we can do with the world is avoid randomness, and figure out ways by which things can be planned. Wisdom like this has its effect on seventeen-year-old freshmen, and from then on Sussman was hooked.\n",
    "\n",
    "- SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
