{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data: past present future\n",
    "## lab 10b: trees and forests\n",
    "\n",
    "### supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use\n",
    "plt.rcParams['figure.figsize'] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrate the standard scikit pipeline for training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Hepatitis\n",
    "\n",
    "The attributes are:\n",
    "\n",
    "1. Class: DIE, LIVE\n",
    "2. AGE: 10, 20, 30, 40, 50, 60, 70, 80\n",
    "3. SEX: male, female\n",
    "4. STEROID: no, yes\n",
    "5. ANTIVIRALS: no, yes\n",
    "6. FATIGUE: no, yes\n",
    "7. MALAISE: no, yes\n",
    "8. ANOREXIA: no, yes\n",
    "9. LIVER BIG: no, yes\n",
    "10. LIVER FIRM: no, yes\n",
    "11. SPLEEN PALPABLE: no, yes\n",
    "12. SPIDERS: no, yes\n",
    "13. ASCITES: no, yes\n",
    "14. VARICES: no, yes\n",
    "15. BILIRUBIN: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00\n",
    "16. ALK PHOSPHATE: 33, 80, 120, 160, 200, 250\n",
    "17. SGOT: 13, 100, 200, 300, 400, 500,\n",
    "18. ALBUMIN: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0\n",
    "19. PROTIME: 10, 20, 30, 40, 50, 60, 70, 80, 90\n",
    "20. HISTOLOGY: no, yes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names=['CLASS','AGE','SEX','STEROID','ANTIVIRALS','FATIGUE','MALAISE','ANOREXIA','LIVER_BIG','LIVER_FIRM','SPLEEN_PALPABLE','SPIDERS','ASCITES','VARICES','BILIRUBIN','ALK_PHOSPHATE','SGOT','ALBUMIN','PROTIME','HISTOLOGY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hep_data=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/hepatitis/hepatitis.data', sep=',', header=None, na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# works better if extract from pandas dataframe\n",
    "# separate the existing classification (the diagnosis) from the features tested\n",
    "hep_data_array=hep_data.values\n",
    "y = hep_data_array[:,0]   #diagnosis\n",
    "X = hep_data_array[:,1:19]  #features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has a problem: lots of question marks, imported as \"NaN\"s. scikit learn no friend of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dodgy data munging!\n",
    "#impute values to missing ones by using mean values of testing results. This may be a total garbage move. Is it?\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X_imputed=imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pick out a subset of our data to \"train\" the model, and a subset to \"test\" the model. \n",
    "\n",
    "There are loads of ways to do this, but `sklearn` offers a simple function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.45, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-0461a7d3e247>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-0461a7d3e247>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Now we set up our classifier and fit the data using it.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Now we set up our classifier and fit the data using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier() #set up classifier, with all **default** values\n",
    "clf=dt.fit(X_train, y_train) #fit on all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can do a little bit of testing to see how well the classifier predicts using cross validation of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_train = cross_val_score(clf, X_train, y_train)\n",
    "scores_test = cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are cool in part because they \"rate an A+ on interpretability\" according to Breiman.\n",
    "\n",
    "Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this may take FOREVAH in class\n",
    "\n",
    "!conda install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!conda install -y pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show us the graph of the trees\n",
    "\n",
    "from IPython.display import Image \n",
    "import pydotplus \n",
    "from sklearn import tree\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=names[1:19],  \n",
    "                     class_names=['die', 'live'],  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is gini, I hear you cry. \n",
    "\n",
    "It's the *splitting criterion* that the decision tree algorithm defaults to.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)\n",
    "\n",
    "\"Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\"\n",
    "\n",
    "To compute Gini impurity for a set of items with $J$ classes, suppose $i \\in \\{1, 2, ...,J\\}$, and let $f_i$ be the fraction of items labeled with class $i$ in the set.\n",
    "\n",
    "$I_{G}(f) = \\sum_{i=1}^{J} f_i (1-f_i) = \\sum_{i=1}^{J} (f_i - {f_i}^2) = \\sum_{i=1}^J f_i - \\sum_{i=1}^{J} {f_i}^2\n",
    " = 1 - \\sum^{J}_{i=1} {f_i}^{2} = \\sum_{i\\neq k}f_i f_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise to the reader:\n",
    "\n",
    "trees are very prone to *overfitting* \n",
    "\n",
    "![overfitting](http://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_001.png)\n",
    "\n",
    "\n",
    "among the many techniques for dealing with this:\n",
    "\n",
    "- constrain the depth of the trees using `max_depth=` \n",
    "- reduce the number of features training on\n",
    "\n",
    "\n",
    "Take five minutes to change the default settings--can you make a more predictive decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DecisionTreeClassifier??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "dramatically increase predictive power at the cost of interpretability\n",
    "\n",
    "combine \n",
    "\n",
    "![forests](https://dimensionless.in/wp-content/uploads/RandomForest_blog_files/figure-html/voting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "#kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "forest = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "clf=forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breiman:\n",
    "\n",
    ">...forests are A+ predictors But their mechanism for producing a prediction is difficult to understand. Trying to delve into the tangled web that generated a plurality vote from 100 trees is a Herculean task. So on interpretability they rate an F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
