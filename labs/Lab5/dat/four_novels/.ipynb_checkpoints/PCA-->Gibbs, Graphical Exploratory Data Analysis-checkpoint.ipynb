{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Goals\n",
    "The purpose of this notebook is to first perform covariance PCA on N novels, with the additional option of decomposing any given novel into a series of \"chunk\" subsets of W words each. Gibbs sampling is then used to identify probability distribution densities for each novel. Novels can be clustered empirically from the overlap of these probability densities. Special attention will be paid to how the size of the subset affects the probability density for each novel. \n",
    "\n",
    "NOTE: Older than PCA-variance notebook. (What is contained here is defined as functions there.) \n",
    "\n",
    "\n",
    "\n",
    "# Notebook Status: \n",
    "\n",
    "## LOOP 0 (PREPROCESSING/ TOKENIZING / CHUNCKING):\n",
    "NOTE: Needs to be in same folder as documents to process.   \n",
    "  - WHOLE TEXTS (i.e., option = 0): OK\n",
    "  - CHUNKED TEXTS (i.e., option =1): unverified (2/1/2017)\n",
    "  \n",
    "\n",
    "## LOOP 1 (PCA of whole and chunks):\n",
    "  - PCA for whole text\n",
    "  - PCA for N chunks\n",
    " \n",
    "## LOOP 2 (GIBBS SAMPLING ON TEXTS):\n",
    "  - Broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FOR PCA\n",
    "import os\n",
    "from os import walk, getcwd, listdir\n",
    "import numpy as np\n",
    "import nltk  #for tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib #to set rcParams to change graph size\n",
    "import time\n",
    "\n",
    "#FOR GIBBS\n",
    "from random import randint\n",
    "import scipy.stats\n",
    "#import math as m\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS (Those not yet implemented in code are commented out)\n",
    "np.random.seed(1) #seed random number generator so \"random\" results are repeatable\n",
    "\n",
    "## Chunking Parameters ----------------------\n",
    "chunk_corpus = 1            # 0 = don't chunk texts; 1 = do chunk texts\n",
    "chunk_size_used = 5000    # number of words per chunk (but will give >chunk_size final word chunks)\n",
    "#chunk_size_interval = 100 # interval of chunking when doing experiments with multiple chunk sizes; \n",
    "#number_of_chunk_size_experiments = 10  # set equal to chunk_size_used if you only want to do experiment for one chunk size\n",
    "\n",
    "## Word Count Parameters --------------------\n",
    "#number_of_MFW_window_experiments = 1 #If set equal to 1, just uses \"Number_of_MFWs_used\"\n",
    "#Number_of_MFWs_used = 25  #number of MFWs used to plot PCA graphs\n",
    "#MFW_interval = 100        # interval increase of MFW window used when doing experiments w/...\n",
    "                           # multiple MFW windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## FUNCTIONS USED TO GET WORD FREQUENCIES FROM TEXTS\n",
    "def count_words(list_to_search): #uses single_type_count() to count each token\n",
    "    unique_words = set(list_to_search)\n",
    "    word_counts = {}\n",
    "    for word in unique_words:\n",
    "        word_counts[word] = single_type_count(word, list_to_search)\n",
    "    return word_counts # dict w/ word counts\n",
    "\n",
    "def single_type_count(token_to_count, list_to_search): #counts up all tokens of a type\n",
    "    number_of_tokens = 0                            \n",
    "    for token in list_to_search:                   \n",
    "        if token == token_to_count:                 \n",
    "            number_of_tokens += 1                   \n",
    "    return number_of_tokens #returns int \n",
    "\n",
    "def total_number_of_words(dict_of_word_counts): #for use with token_counts\n",
    "    number_of_words = 0\n",
    "    for word in dict_of_word_counts:\n",
    "        number_of_words = number_of_words + dict_of_word_counts[word]\n",
    "    return number_of_words #returns int\n",
    "\n",
    "def total_number_of_words_in_corpus(list_of_total_word_counts):\n",
    "    total_number_of_words = 0\n",
    "    for total in range(0,len(list_of_total_word_counts)):\n",
    "        total_number_of_words = total_number_of_words + list_of_total_word_counts[total]\n",
    "    return total_number_of_words #returns int\n",
    "\n",
    "def get_word_frequencies(dict_of_words_with_counts, total_number_of_words_in_text):\n",
    "    word_freq = {}\n",
    "    for word in dict_of_words_with_counts:\n",
    "        word_freq[word] = dict_of_words_with_counts[word]/total_number_of_words_in_text\n",
    "    return word_freq # dict with word w/ normalized frequencies\n",
    "\n",
    "## FUNCTION TO CHUNK A TEXT OF ANY SIZE\n",
    "# Note that \"wordlist\" needs to be a list of words *in the order* of the original text;\n",
    "# also, chunk_text returns the number of chunks used for a particular text in addition\n",
    "# to a list of text chunks\n",
    "def chunk_text(wordlist, words_per_chunk): #chunk text; where last chunk <= words_per_chunk\n",
    "    if words_per_chunk == 0:\n",
    "        return\n",
    "    number_of_chunks, size_of_final_chunk = divmod(len(wordlist), words_per_chunk)\n",
    "    number_of_chunks = number_of_chunks + 1 #we add one because divmod does include final chunk\n",
    "    temp = [] #We build a list of chunks using this list\n",
    "    list_of_text_chunks = [temp + wordlist[words_per_chunk*chunk:words_per_chunk*(chunk+1)] for chunk in range(0, number_of_chunks)]\n",
    "    return list_of_text_chunks, number_of_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## READ ALL DOCUMENTS IN DIRECTORY AS STRINGS IN A LIST\n",
    "alltexts = [ filename for filename in listdir(getcwd()) if filename.endswith('.txt')] #get filenames in current directory ending in \".txt\"\n",
    "wordlist = [] #each element is the word list for alltexts[document]\n",
    "novelnames = []  #each element is the name of the text\n",
    "\n",
    "for document in range(0, len(alltexts)):                   # Loop through all files in directory\n",
    "    root, ext = os.path.splitext(alltexts[document])       # Select file extension for particular file \"x\" in the list \"allFilesInDirectory\"\n",
    "    if (ext == '.txt'):    \n",
    "        text = open(str(alltexts[document]), \"r\").read()   #read file\n",
    "        novelnames.append(root)                            #create list of text names\n",
    "        \n",
    "## TOKENIZE WORDS FOR ALL DOCUMENTS, i.e., [[text1_words],[text2_words],...]\n",
    "        text = text.lower()\n",
    "        wordlist.append(nltk.tokenize.word_tokenize(text))     #tokenize file (a poor tokenizer)\n",
    "        #wordlist.append(text.lower().split())                 #tokenize file (a terrible tokenizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## BREAK UP TOKENIZED TEXTS INTO CHUNKS\n",
    "# The following code produces two lists: (1) a list of word chunks for the entire corpus\n",
    "# (e.g., [[word_chunk_list_1], [word_chunk_list_2],...]) \n",
    "# AND (2) a list of the starting and ending chunks associated with each text, where the \n",
    "# index number for each text is the same as the text in \"novelnames\" list\n",
    "# (e.g., [[staring_chunk_#_for_text_1 , ending_chunk_#_for_text_1],...])\n",
    "\n",
    "document_chunk_index = [] # a list such that document_chunk_index[0][0] is the index \n",
    "                          # for the 1st chunk of the 1st document and document_chunk_index[0][1]\n",
    "                          # is index for the final chunk of the 1st document; will remain empty\n",
    "                          # if no chunking is performed (i.e., if chunk_text = 0)\n",
    "\n",
    "wordlist_of_chunks = []  # a list of chunk wordlists for entire corpus\n",
    "  \n",
    "if chunk_corpus == 1: #turn text chunking on or off in parameters section at beginning of code\n",
    "    for document in range(0, len(alltexts)):        \n",
    "            wordlists_for_all_chunks_in_a_text, number_of_chunks = chunk_text(wordlist[document], chunk_size_used)\n",
    "            wordlist_of_chunks = wordlist_of_chunks + wordlists_for_all_chunks_in_a_text #.concatenate(wordlists_for_all_chunks_in_a_text) #add chunks to corpus list of chunks\n",
    "            if document == 0:\n",
    "                document_chunk_index = [[0, number_of_chunks-1]]\n",
    "            else:\n",
    "                document_chunk_index.append([document_chunk_index[document-1][1]+1,document_chunk_index[document-1][1]+number_of_chunks])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TESTS FOR WORD CHUNKING USING 3 WOOLF NOVELS\n",
    "#print(\"wordlist_of_chunks\")                \n",
    "#print(wordlist_of_chunks)\n",
    "#print(\"length of wordlist of chunks\")\n",
    "#print(len(wordlist_of_chunks))\n",
    "#print(\"Number of words in first chunk\")\n",
    "#print(len(wordlist_of_chunks[0]))\n",
    "#print(\"document_chunk_index\")\n",
    "#print(document_chunk_index)\n",
    "#print(\"16, 56, 89\")\n",
    "#print(str(len(wordlist_of_chunks[16]))+ \", \"+str(len(wordlist_of_chunks[56]))+\", \"+ str(len(wordlist_of_chunks[89])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CALCULATE WORD COUNTS, i.e., [{text1_word_counts}, {text2_word_counts},...]\n",
    "t0 = time.clock() #grab the time; this used to track how long word counting takes\n",
    "\n",
    "# FIRST, CALCULATE WORD COUNTS FOR INDIVIDUAL TEXTS IN CORPUS\n",
    "word_counts = []\n",
    "total_word_counts = []\n",
    "for text in range(0, len(wordlist)):\n",
    "    word_counts.append(count_words(wordlist[text]))\n",
    "    total_word_counts.append(total_number_of_words(word_counts[text]))\n",
    "\n",
    "# We also need to calculate total words in corpus\n",
    "corpus_word_count = total_number_of_words_in_corpus(total_word_counts)\n",
    "t1 = time.clock() #grab the time; this used to track how long word counting takes\n",
    "\n",
    "# SECOND, IF CHUNKING TEXTS, CALCULATE WORD COUNTS FOR INDIVIDUAL CHUNKS BY REPEATING THE ABOVE PROCEDURE. \n",
    "word_counts_for_chunks = []\n",
    "total_word_counts_for_chunks = [] \n",
    "corpus_word_count_for_chunks = 0\n",
    "if chunk_corpus ==1:\n",
    "        for chunk in range(0, len(wordlist_of_chunks)):\n",
    "            word_counts_for_chunks.append(count_words(wordlist_of_chunks[chunk]))\n",
    "            total_word_counts_for_chunks.append(total_number_of_words(word_counts_for_chunks[chunk]))\n",
    "\n",
    "# We also need to calculate total words in corpus for chunks\n",
    "corpus_word_count_for_chunks = 0 # We need this variable to be global so I arbitrarily initialize it to zero\n",
    "if chunk_corpus ==1:\n",
    "    corpus_word_count_for_chunks = total_number_of_words_in_corpus(total_word_counts_for_chunks)\n",
    "t2 = time.clock() #grab the time; this used to track how long word counting takes\n",
    "\n",
    "#NOTE: WORD COUNT FOR INDIVIDUAL TEXTS AND CHUNKS SHOULD BE THE SAME!!\n",
    "if chunk_corpus ==1:\n",
    "    if corpus_word_count != corpus_word_count_for_chunks:\n",
    "        print(\"FATAL ERROR: Total word counts for texts and chunks are not equal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##TESTS FOR WORD COUNTING USING 3 WOOLF NOVELS\n",
    "print(\"corpus_word_count: \"+ str(corpus_word_count))                \n",
    "print(\"corpus_word_count_for_chunks: \" + str(corpus_word_count_for_chunks))\n",
    "# Output time it takes to run code block\n",
    "print(\"Time to count words of texts:\"+ str((t1-t0)/60.0)+\" minute(s)\") \n",
    "print(\"Time to count words of chunks:\"+ str((t2-t1)/60.0)+\" minute(s)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CALCULATE WORD FREQUENCIES (RELATIVE TO ENTIRE CORPUS)\n",
    "\n",
    "# First, for individual texts...\n",
    "word_frequencies = []\n",
    "for text_word_counts in range(0, len(word_counts)):\n",
    "    word_frequencies.append(get_word_frequencies(word_counts[text_word_counts],corpus_word_count))\n",
    "\n",
    "#Second, for individual chunks...\n",
    "word_frequencies_for_chunks = []\n",
    "if chunk_corpus == 1: \n",
    "    for chunk_word_counts in range(0, len(word_counts_for_chunks)):\n",
    "            word_frequencies_for_chunks.append(get_word_frequencies(word_counts_for_chunks[chunk_word_counts],corpus_word_count_for_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FIRST, EXAMINE MFWs (OR LFWs) IN A READABLE FORMAT USING PANDAS DATAFRAMES FOR INDIVIDUAL TEXTS IN CORPUS\n",
    "readable_word_frequencies = pd.DataFrame(word_frequencies).T\n",
    "text_to_examine = 0 # column identifer for a particular text; full list in \"novelnames\"\n",
    "MFW = readable_word_frequencies.sort_values([text_to_examine], ascending = False)\n",
    "MFW = MFW.fillna(0)\n",
    "#print(novelnames) #uncomment to display list of texts\n",
    "#MFW.head(25) #Display first 25 words for MFW\n",
    "#MFW #uncomment to display MFW relative to a single text; WARNING: this list is millions of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SECOND, EXAMINE MFWs (OR LFWs) FOR INDIVIDUAL CHUNKS (IF chunk_corpus == 1)\n",
    "if chunk_corpus == 1:\n",
    "    readable_word_frequencies_for_chunks = pd.DataFrame(word_frequencies_for_chunks).T\n",
    "    chunk_to_examine = 0 # column identifer for a particular text; full list in \"novelnames\"\n",
    "    MFW_for_chunks = readable_word_frequencies_for_chunks.sort_values([chunk_to_examine], ascending = False) #sort using MFW of novelnames-text_to_examine] text\n",
    "    MFW_for_chunks = MFW_for_chunks.fillna(0)\n",
    "    #FOR TESTING\n",
    "    #print(novelnames) #uncomment to display list of texts\n",
    "    #print(document_chunk_index) #uncomment to display index for chunks for each text\n",
    "    #print(MFW_for_chunks.head(25)) #Display first 25 MFWords for all novels in comparison to novelnames[text_to_examine]\n",
    "    #MFW #uncomment to display MFW relative to a single text; WARNING: this list may be millions of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TRANSFORM WORD FREQUENCIES LIST FOR PCA\n",
    "# To do PCA with scikit-learn, we need a matrix where each word (i.e., each dimension) \n",
    "# gets its own nested list in which each text is an element in that word list\n",
    "print(\"corpus word count:\" + str(corpus_word_count))\n",
    "Number_of_MFWs_used = 444564 #used in next line to pull N MFWs from MFW dataframe\n",
    "MFWlist_array_for_PCA = (MFW.head(Number_of_MFWs_used).as_matrix()).T #convert from dataframe to numpy array (for texts)\n",
    "MFWlist_array_for_PCA_for_chunks = (MFW_for_chunks.head(Number_of_MFWs_used).as_matrix()).T #convert from dataframe to numpy array (for chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## FIRST, PERFORM PCA AND GENERATE PCA GRAPH FOR ENTIRE TEXTS -------------------------\n",
    "## ------------------------------------------------------------------------------------\n",
    "\n",
    "## (0) We want to autogenerate colors; one for each text and/or chunk. To do this, we'll\n",
    "## take the built in dictionary in matplotlib. Sadly, we can't just iterate through \n",
    "## this list because it will pick colors that all look very similar so\n",
    "## we choose values at random from this list, being sure to use no color twice... \n",
    "\n",
    "#Generate color scheme for text plots by picking colors randomly\n",
    "graph_colors = list(matplotlib.colors.cnames.items()) #colors[0][0], colors[1][0],...colors[150][0] all return defined color names\n",
    "colors_for_texts = [] \n",
    "numbers_used = []\n",
    "for text in range(0, len(alltexts)):\n",
    "    iterate = 1\n",
    "    while iterate == 1:\n",
    "        random_int = randint(0,150)  \n",
    "        if random_int not in numbers_used:\n",
    "            numbers_used = numbers_used + [random_int]\n",
    "            colors_for_texts = colors_for_texts + [graph_colors[random_int][0]]\n",
    "            iterate = 0\n",
    "            \n",
    "## (1) generate data points of PCA from MFW word list (this is where PCA is performed)\n",
    "pca = sklearnPCA(n_components=2)\n",
    "pca_coordinates = pca.fit_transform(MFWlist_array_for_PCA) #array of x- & y-coordinates \n",
    "PCA_coordinates_for_texts = pca_coordinates # will be gibbs sampled \n",
    "\n",
    "## (2) plot PCA data\n",
    "for text in range(0, len(PCA_coordinates_for_texts)):\n",
    "    plt.plot(pca_coordinates[text,0],pca_coordinates[text,1], \n",
    "        'o', markersize=7, color=colors_for_texts[text], alpha=0.5, label=novelnames[text])\n",
    "#plt.plot(sklearn_pca_coordinates[0,0], sklearn_pca_coordinates[0,1],\n",
    "#        'o', markersize=7, color=colors_for_texts[0], alpha=0.5, label=novelnames[0])\n",
    "#plt.plot(sklearn_pca_coordinates[1,0], sklearn_pca_coordinates[1,1],\n",
    "#         '^', markersize=7, color=colors_for_texts[1], alpha=0.5, label=novelnames[1])\n",
    "#plt.plot(sklearn_pca_coordinates[2,0], sklearn_pca_coordinates[2,1],\n",
    "#         '^', markersize=7, color=colors_for_texts[2], alpha=0.5, label=novelnames[2])\n",
    "\n",
    "## (3) Select Graph Display Parameters\n",
    "plt.xlabel('PC 1 ('+str(pca.explained_variance_ratio_[0]*100)+'%)') #x-axis title\n",
    "plt.ylabel('PC 2('+str(pca.explained_variance_ratio_[1]*100)+'%)') #y-axis title\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0,12.0) #size of graph generated in notebook\n",
    "#plt.xlim(x_min, x_max)\n",
    "#plt.ylim(y_min, y_max)\n",
    "#plt.axis('tight') #OR just fit plot around data automatically; but this usually fits *so* closely that it misses data\n",
    "plt.legend()  #generate legend\n",
    "plt.title('PCA for ' + str(len(novelnames)) + ' novels') #title of plot\n",
    "ax = plt.subplot(111) #used in making legend\n",
    "\n",
    "##PLACE LEGEND OUTSIDE OF PLOT\n",
    "# Shrink current axis's height by 10% on the bottom\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 0.9])\n",
    "\n",
    "# Put a legend below current axis\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -.15),\n",
    "          fancybox=True, shadow=False, ncol=5)\n",
    "\n",
    "# Add gridlines \n",
    "plt.grid(b=True, which='major', color='gray', linestyle='dotted')\n",
    "\n",
    "#Plot Graph!\n",
    "plt.show()\n",
    "\n",
    "## PERFORM PCA AND GENERATE PCA GRAPH FOR ALL TEXT CHUNKS  ------------------\n",
    "# (1) generate data points of PCA from MFW word list (this is where PCA is performed)\n",
    "pca = sklearnPCA(n_components=2)\n",
    "pca_coordinates = pca.fit_transform(MFWlist_array_for_PCA_for_chunks) #array of x- & y-coordinates \n",
    "pca_coordinates_for_chunks = pca_coordinates # will be gibbs sampled\n",
    "\n",
    "# (2) plot PCA data\n",
    "for text in range(0, len(novelnames)):\n",
    "    print(chunk)\n",
    "    plt.plot(pca_coordinates[document_chunk_index[text][0]:document_chunk_index[text][1],0], \n",
    "        pca_coordinates[document_chunk_index[text][0]:document_chunk_index[text][1],1],\n",
    "        'o', markersize=7, color=colors_for_texts[text], alpha=0.5, label=novelnames[text])\n",
    "\n",
    "# (3) select Graph Parameters\n",
    "plt.xlabel('PC 1 ('+str(pca.explained_variance_ratio_[0]*100)+'%)') #x-axis title\n",
    "plt.ylabel('PC 2('+str(pca.explained_variance_ratio_[1]*100)+'%)') #y-axis title\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0,12.0) #size of graph generated in notebook\n",
    "#plt.xlim(x_min, x_max)\n",
    "#plt.ylim(y_min, y_max)\n",
    "#plt.axis('tight') #OR just fit plot around data automatically; but this usually fits *so* closely that it excludes data points\n",
    "plt.legend()  #generate legend\n",
    "plt.title('PCA for ' + str(len(novelnames)) + ' novels in chunks of '+str(chunk_size_used)+' words') #title of plot\n",
    "ax = plt.subplot(111) #used in making legend\n",
    "\n",
    "##PLACE LEGEND OUTSIDE OF PLOT\n",
    "# Shrink current axis's height by 10% on the bottom\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 0.9])\n",
    "\n",
    "# Put a legend below current axis\n",
    "ax.legend(loc='lower center', bbox_to_anchor=(0.5, -.15),\n",
    "          fancybox=True, shadow=False, ncol=5)\n",
    "\n",
    "# Add gridlines \n",
    "plt.grid(b=True, which='major', color='gray', linestyle='dotted')\n",
    "\n",
    "#plot graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TESTS\n",
    "print(len(document_chunk_index))\n",
    "print(document_chunk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NOW WE WANT TO DO GIBBS SAMPLING ON THE RESULTING PCA DATA FOR CLUSTERS\n",
    "##------------------------------------------------------------------------\n",
    "\n",
    "## INITIAL PARAMETERS USING PCA CLUSTER DATA\n",
    "#  Note: we can't generate a probability distribution from a point; so the Gibbs sampler\n",
    "#  is sampling chunks only. That said, we can use the average of the chunks for each text\n",
    "#  to obtain the prior for mu prior. \n",
    "\n",
    "## Definitions ------------------------------------------------------------------------------------\n",
    "\n",
    "#  mu: the coordinates for the prior of the center of the gaussians probability densities; \n",
    "#      our prior (i.e., our initial \"guess\") is that this is the average of the chunk coordinates \n",
    "#      for a particular text\n",
    "## Pick prior Mu_i for all texts \n",
    "mu = [] \n",
    "for text in range(0, len(novelnames)):\n",
    "    mu.append([np.mean(pca_coordinates_for_chunks[document_chunk_index[text][0]:document_chunk_index[text][1],0]), \n",
    "             np.mean(pca_coordinates_for_chunks[document_chunk_index[text][0]:document_chunk_index[text][1],1])])\n",
    "mu = np.array(mu)\n",
    "\n",
    "#  sigma: the the variance for each text (i.e., each cluster); we assume that the variance will be\n",
    "#      proportional to the number of chunks in the text such that sigma ~ number_of_chunks/2.\n",
    "#      While this is an approximation, it is not a bad one.\n",
    "\n",
    "## Pick sigmas for all texts\n",
    "#  We \"guess\" that the variance will be proportional to the number of chunks in the text \n",
    "#  such that sigma ~ (1 + number_of_chunks/2)\n",
    "sigma = []\n",
    "for text in range(0, len(novelnames)):\n",
    "    sigma.append([(document_chunk_index[text][1]+1-document_chunk_index[text][0])/2])\n",
    "sigma = np.array(sigma)\n",
    "\n",
    "##  K: number of clusters (i.e., number of texts)\n",
    "K = len(novelnames) #Number of clusters (i.e., number of texts)\n",
    "\n",
    "##  Z_i: text assignments for each chunk (not assumed (even though we know it, but found with gibbs sampling)\n",
    "Z_i = np.random.randint(0, K, size=(len(pca_coordinates_for_chunks),1)) #generate random Z_i assignments\n",
    "\n",
    "## number of iterations to run gibbs sampler\n",
    "N_iterations = 15000 #number of iterations that will be performed for each cluster of chunks (i.e., each text)  \n",
    "\n",
    "## the initial\"guess\" for the variance of the gaussian from which mu is calculated\n",
    "lambda_ = np.array([1.0 for i in range(K)]).reshape(K,-1)\n",
    "#mu = scipy.stats.multivariate_normal.rvs(cov=np.diagflat([lambda_hat[k], lambda_hat[k]]), size=K) #THese are guesses\n",
    "\n",
    "## Total Number of Chunks\n",
    "number_of_chunks = len(PCA_coordinates_for_chunks)\n",
    "\n",
    "#TEST 1\n",
    "print(\"Mu_k\")\n",
    "print(mu)\n",
    "print(\"Sigma_k\")\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
